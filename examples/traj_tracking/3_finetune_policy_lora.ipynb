{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cc48ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import jax\n",
    "import optax\n",
    "from orbax.checkpoint import PyTreeCheckpointer\n",
    "from flax.training.train_state import TrainState\n",
    "from flax.core import freeze, unfreeze\n",
    "\n",
    "from lotf import LOTF_PATH\n",
    "from lotf.algos import bptt\n",
    "from lotf.envs import TrajTrackingStateEnv\n",
    "from lotf.envs.wrappers import MinMaxObservationWrapper, LogWrapper, VecEnv\n",
    "from lotf.modules import MLP, LoraMLP\n",
    "from lotf.objects import Quadrotor, RefTrajNames\n",
    "\n",
    "from lotf.utils.lora import (\n",
    "    lora_only_mask,\n",
    "    partition_params,\n",
    "    recursive_merge,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71dc545",
   "metadata": {},
   "source": [
    "# (LoRA) Finetuning a Trained State-Based Hovering Policy With BPTT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc81526",
   "metadata": {},
   "source": [
    "## 1. Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8850bd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "key = jax.random.key(seed)\n",
    "key_init, key_bptt = jax.random.split(key, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69649248",
   "metadata": {},
   "source": [
    "## 2. Define Simulation Dynamics Config and Training Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e14b8e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulation dynamics config\n",
    "sim_dyn_config = {\n",
    "    \"use_high_fidelity\": False,          # whether to use high-fidelity dynamics in forward simulation\n",
    "    \"use_forward_residual\": False,       # whether to use residual dynamics in forward simulation\n",
    "}\n",
    "\n",
    "# training parameters\n",
    "num_envs = 200\n",
    "max_epochs = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3116fb",
   "metadata": {},
   "source": [
    "## 3. Create Quadrotor Object and Simulation Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15bce161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== env info ======\n",
      "action_dim: 4\n",
      "obs_dim: 27\n"
     ]
    }
   ],
   "source": [
    "# simulation parameters\n",
    "sim_dt = 0.02\n",
    "max_sim_time = 5.0\n",
    "\n",
    "# reference trajectory\n",
    "ref_traj_name = RefTrajNames.FIG8\n",
    "\n",
    "# quadrotor object\n",
    "quad_obj = Quadrotor.from_name(\"example_quad\", sim_dyn_config)\n",
    "\n",
    "# simulation environment\n",
    "env = TrajTrackingStateEnv(\n",
    "    max_steps_in_episode=int(max_sim_time / sim_dt),\n",
    "    dt=sim_dt,\n",
    "    delay=0.04,\n",
    "    yaw_scale=0.1,\n",
    "    pitch_roll_scale=0.1,\n",
    "    position_std=0.05,\n",
    "    velocity_std=0.05,\n",
    "    omega_std=0.05,\n",
    "    quad_obj=quad_obj,\n",
    "    ref_traj_name=ref_traj_name,\n",
    ")\n",
    "\n",
    "# apply min-max observation wrapper\n",
    "env = MinMaxObservationWrapper(env)\n",
    "\n",
    "# get dimensions\n",
    "action_dim = env.action_space.shape[0]\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "\n",
    "# apply additional wrappers\n",
    "env = LogWrapper(env)\n",
    "env = VecEnv(env)\n",
    "\n",
    "print(\"====== env info ======\")\n",
    "print(f\"action_dim: {action_dim}\")\n",
    "print(f\"obs_dim: {obs_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4340d218",
   "metadata": {},
   "source": [
    "## 4. Load Base Policy Parameters, Create Optimizer and Train State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9e6ae57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/orbax/checkpoint/type_handlers.py:1330: UserWarning: Couldn't find sharding info under RestoreArgs. Populating sharding info from sharding file. Please note restoration time will be slightly increased due to reading from file instead of directly from RestoreArgs. Note also that this option is unsafe when restoring on a different topology than the checkpoint was saved with.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "policy_name = \"traj_tracking_params\"\n",
    "\n",
    "# policy network and init parameters\n",
    "base_policy_net = MLP(\n",
    "    [obs_dim, 512, 512, action_dim],\n",
    "    initial_scale=0.01,\n",
    "    action_bias=env.hovering_action,\n",
    ")\n",
    "path = LOTF_PATH + \"/../checkpoints/policy/\" + policy_name\n",
    "ckptr = PyTreeCheckpointer()\n",
    "base_policy_params = ckptr.restore(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d0974d",
   "metadata": {},
   "source": [
    "## 5. Define LoRA Policy Network, Create Optimizer and Train State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "462f6fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_ranks = [1, 1, 1]\n",
    "lora_alpha = 1.0\n",
    "\n",
    "# LoRA policy network\n",
    "policy_net = LoraMLP(base_mlp=base_policy_net, lora_ranks=lora_ranks, lora_alpha=lora_alpha)\n",
    "policy_params = policy_net.initialize_with_base(key_init, base_policy_params)\n",
    "\n",
    "mask = lora_only_mask(policy_params)\n",
    "frozen_params, trainable_params = partition_params(policy_params, mask)\n",
    "def apply_combined(params, x):\n",
    "    full_params = freeze(recursive_merge(unfreeze(frozen_params), unfreeze(params)))\n",
    "    return policy_net.apply(full_params, x)\n",
    "\n",
    "# optimizer and train state\n",
    "tx = optax.adam(learning_rate=1e-3)\n",
    "train_state = TrainState.create(\n",
    "    apply_fn=apply_combined, params=trainable_params, tx=tx\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c691b6",
   "metadata": {},
   "source": [
    "## 5. Load Residual Dynamics Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2d806de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/orbax/checkpoint/type_handlers.py:1330: UserWarning: Couldn't find sharding info under RestoreArgs. Populating sharding info from sharding file. Please note restoration time will be slightly increased due to reading from file instead of directly from RestoreArgs. Note also that this option is unsafe when restoring on a different topology than the checkpoint was saved with.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "residual_dynamics_name = \"example_params\"\n",
    "\n",
    "path = LOTF_PATH + \"/../checkpoints/residual_dynamics/\" + residual_dynamics_name\n",
    "ckptr = PyTreeCheckpointer()\n",
    "dummy_residual_params = ckptr.restore(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041f685b",
   "metadata": {},
   "source": [
    "## 6. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f89cc27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Grad max: 5.3454\n",
      "Episode: 0, Loss: 1.57\n",
      "Episode: 10, Grad max: 2.1246\n",
      "Episode: 10, Loss: 1.55\n",
      "Episode: 20, Grad max: 1.9112\n",
      "Episode: 20, Loss: 1.54\n",
      "Episode: 30, Grad max: 1.4380\n",
      "Episode: 30, Loss: 1.56\n",
      "Episode: 40, Grad max: 0.8114\n",
      "Episode: 40, Loss: 1.60\n",
      "Episode: 50, Grad max: 1.8587\n",
      "Episode: 50, Loss: 1.48\n",
      "Episode: 60, Grad max: 1.1978\n",
      "Episode: 60, Loss: 1.74\n",
      "Episode: 70, Grad max: 3.3141\n",
      "Episode: 70, Loss: 1.77\n",
      "Episode: 80, Grad max: 0.4605\n",
      "Episode: 80, Loss: 1.42\n",
      "Episode: 90, Grad max: 0.8720\n",
      "Episode: 90, Loss: 1.48\n",
      "Episode: 100, Grad max: 0.8825\n",
      "Episode: 100, Loss: 1.45\n",
      "Episode: 110, Grad max: 1.9366\n",
      "Episode: 110, Loss: 1.52\n",
      "Episode: 120, Grad max: 0.8636\n",
      "Episode: 120, Loss: 1.41\n",
      "Episode: 130, Grad max: 1.0665\n",
      "Episode: 130, Loss: 1.54\n",
      "Episode: 140, Grad max: 2.6608\n",
      "Episode: 140, Loss: 1.26\n",
      "Episode: 150, Grad max: 1.3239\n",
      "Episode: 150, Loss: 1.40\n",
      "Episode: 160, Grad max: 0.6794\n",
      "Episode: 160, Loss: 1.39\n",
      "Episode: 170, Grad max: 1.4245\n",
      "Episode: 170, Loss: 1.36\n",
      "Episode: 180, Grad max: 1.2079\n",
      "Episode: 180, Loss: 1.41\n",
      "Episode: 190, Grad max: 2.4720\n",
      "Episode: 190, Loss: 1.34\n",
      "Compile + Training time: 13.556495666503906\n"
     ]
    }
   ],
   "source": [
    "# intialize environments\n",
    "key_bptt, key_ = jax.random.split(key_bptt)\n",
    "key_reset = jax.random.split(key_, num_envs)\n",
    "init_env_state, init_obs = env.reset(key_reset, None)\n",
    "\n",
    "# training loop\n",
    "time_start = time.time()\n",
    "res_dict = bptt.train(\n",
    "    env,\n",
    "    init_env_state,\n",
    "    init_obs,\n",
    "    train_state,\n",
    "    num_epochs=max_epochs,\n",
    "    num_steps_per_epoch=env.max_steps_in_episode,\n",
    "    num_envs=num_envs,\n",
    "    res_model_params=dummy_residual_params,\n",
    "    key=key_bptt,\n",
    ")\n",
    "time_train_compile = time.time() - time_start\n",
    "print(f\"Compile + Training time: {time_train_compile}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
