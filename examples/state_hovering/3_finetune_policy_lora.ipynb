{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc48ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import jax\n",
    "import optax\n",
    "from orbax.checkpoint import PyTreeCheckpointer\n",
    "from flax.training.train_state import TrainState\n",
    "from flax.core import freeze, unfreeze\n",
    "\n",
    "from lotf import LOTF_PATH\n",
    "from lotf.algos import bptt\n",
    "from lotf.envs import HoveringStateEnv\n",
    "from lotf.envs.wrappers import MinMaxObservationWrapper, LogWrapper, VecEnv\n",
    "from lotf.modules import MLP, LoraMLP\n",
    "from lotf.objects import Quadrotor\n",
    "\n",
    "from lotf.utils.lora import (\n",
    "    lora_only_mask,\n",
    "    partition_params,\n",
    "    recursive_merge,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71dc545",
   "metadata": {},
   "source": [
    "# (LoRA) Finetuning a Trained State-Based Hovering Policy With BPTT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc81526",
   "metadata": {},
   "source": [
    "## 1. Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8850bd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "key = jax.random.key(seed)\n",
    "key_init, key_bptt = jax.random.split(key, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69649248",
   "metadata": {},
   "source": [
    "## 2. Define Simulation Dynamics Config and Training Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e14b8e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulation dynamics config\n",
    "sim_dyn_config = {\n",
    "    \"use_high_fidelity\": False,          # whether to use high-fidelity dynamics in forward simulation\n",
    "    \"use_forward_residual\": False,       # whether to use residual dynamics in forward simulation\n",
    "}\n",
    "\n",
    "# training parameters\n",
    "num_envs = 200\n",
    "max_epochs = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3116fb",
   "metadata": {},
   "source": [
    "## 3. Create Quadrotor Object and Simulation Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15bce161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== env info ======\n",
      "action_dim: 4\n",
      "obs_dim: 27\n",
      "target hover goal: [1.5 0.  1.5]\n"
     ]
    }
   ],
   "source": [
    "# simulation parameters\n",
    "sim_dt = 0.02\n",
    "max_sim_time = 3.0\n",
    "\n",
    "# quadrotor object\n",
    "quad_obj = Quadrotor.from_name(\"example_quad\", sim_dyn_config)\n",
    "\n",
    "# simulation environment\n",
    "env = HoveringStateEnv(\n",
    "    max_steps_in_episode=int(max_sim_time / sim_dt),\n",
    "    dt=sim_dt,\n",
    "    delay=0.04,\n",
    "    yaw_scale=1.0,\n",
    "    pitch_roll_scale=0.1,\n",
    "    velocity_std=0.1,\n",
    "    omega_std=0.1,\n",
    "    quad_obj=quad_obj,\n",
    "    reward_sharpness=3.0,\n",
    "    action_penalty_weight=0.5,\n",
    "    margin=0.5,\n",
    "    hover_target=[1.5, 0.0, 1.5],\n",
    ")\n",
    "\n",
    "# apply min-max observation wrapper\n",
    "env = MinMaxObservationWrapper(env)\n",
    "\n",
    "# get dimensions\n",
    "action_dim = env.action_space.shape[0]\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "\n",
    "# apply additional wrappers\n",
    "env = LogWrapper(env)\n",
    "env = VecEnv(env)\n",
    "\n",
    "print(\"====== env info ======\")\n",
    "print(f\"action_dim: {action_dim}\")\n",
    "print(f\"obs_dim: {obs_dim}\")\n",
    "print(f\"target hover goal: {env.goal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4340d218",
   "metadata": {},
   "source": [
    "## 4. Load Base Policy Parameters, Create Optimizer and Train State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9e6ae57",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_name = \"state_hovering_params\"\n",
    "\n",
    "# policy network and init parameters\n",
    "base_policy_net = MLP(\n",
    "    [obs_dim, 512, 512, action_dim],\n",
    "    initial_scale=0.01,\n",
    "    action_bias=env.hovering_action,\n",
    ")\n",
    "path = LOTF_PATH + \"/../checkpoints/policy/\" + policy_name\n",
    "ckptr = PyTreeCheckpointer()\n",
    "base_policy_params = ckptr.restore(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d0974d",
   "metadata": {},
   "source": [
    "## 5. Define LoRA Policy Network, Create Optimizer and Train State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "462f6fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_ranks = [1, 1, 1]\n",
    "lora_alpha = 1.0\n",
    "\n",
    "# LoRA policy network\n",
    "policy_net = LoraMLP(base_mlp=base_policy_net, lora_ranks=lora_ranks, lora_alpha=lora_alpha)\n",
    "policy_params = policy_net.initialize_with_base(key_init, base_policy_params)\n",
    "\n",
    "mask = lora_only_mask(policy_params)\n",
    "frozen_params, trainable_params = partition_params(policy_params, mask)\n",
    "def apply_combined(params, x):\n",
    "    full_params = freeze(recursive_merge(unfreeze(frozen_params), unfreeze(params)))\n",
    "    return policy_net.apply(full_params, x)\n",
    "\n",
    "# optimizer and train state\n",
    "tx = optax.adam(learning_rate=1e-3)\n",
    "train_state = TrainState.create(\n",
    "    apply_fn=apply_combined, params=trainable_params, tx=tx\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c691b6",
   "metadata": {},
   "source": [
    "## 5. Load Residual Dynamics Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2d806de",
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_dynamics_name = \"example_params\"\n",
    "\n",
    "path = LOTF_PATH + \"/../checkpoints/residual_dynamics/\" + residual_dynamics_name\n",
    "ckptr = PyTreeCheckpointer()\n",
    "dummy_residual_params = ckptr.restore(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041f685b",
   "metadata": {},
   "source": [
    "## 6. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f89cc27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Grad max: 0.0198\n",
      "Episode: 0, Loss: 0.78\n",
      "Episode: 10, Grad max: 0.0151\n",
      "Episode: 10, Loss: 0.73\n",
      "Episode: 20, Grad max: 0.0291\n",
      "Episode: 20, Loss: 0.72\n",
      "Episode: 30, Grad max: 0.0227\n",
      "Episode: 30, Loss: 0.77\n",
      "Episode: 40, Grad max: 0.0171\n",
      "Episode: 40, Loss: 0.75\n",
      "Episode: 50, Grad max: 0.0122\n",
      "Episode: 50, Loss: 0.74\n",
      "Episode: 60, Grad max: 0.0112\n",
      "Episode: 60, Loss: 0.76\n",
      "Episode: 70, Grad max: 0.0271\n",
      "Episode: 70, Loss: 0.74\n",
      "Episode: 80, Grad max: 0.0122\n",
      "Episode: 80, Loss: 0.76\n",
      "Episode: 90, Grad max: 0.0396\n",
      "Episode: 90, Loss: 0.76\n",
      "Episode: 100, Grad max: 0.0484\n",
      "Episode: 100, Loss: 0.77\n",
      "Episode: 110, Grad max: 0.0358\n",
      "Episode: 110, Loss: 0.81\n",
      "Episode: 120, Grad max: 0.0264\n",
      "Episode: 120, Loss: 0.75\n",
      "Episode: 130, Grad max: 0.0117\n",
      "Episode: 130, Loss: 0.75\n",
      "Episode: 140, Grad max: 0.0230\n",
      "Episode: 140, Loss: 0.76\n",
      "Episode: 150, Grad max: 0.0325\n",
      "Episode: 150, Loss: 0.75\n",
      "Episode: 160, Grad max: 0.0159\n",
      "Episode: 160, Loss: 0.76\n",
      "Episode: 170, Grad max: 0.0143\n",
      "Episode: 170, Loss: 0.73\n",
      "Episode: 180, Grad max: 0.0268\n",
      "Episode: 180, Loss: 0.76\n",
      "Episode: 190, Grad max: 0.0317\n",
      "Episode: 190, Loss: 0.74\n",
      "Compile + Training time: 12.964870691299438\n"
     ]
    }
   ],
   "source": [
    "# intialize environments\n",
    "key_bptt, key_ = jax.random.split(key_bptt)\n",
    "key_reset = jax.random.split(key_, num_envs)\n",
    "init_env_state, init_obs = env.reset(key_reset, None)\n",
    "\n",
    "# training loop\n",
    "time_start = time.time()\n",
    "res_dict = bptt.train(\n",
    "    env,\n",
    "    init_env_state,\n",
    "    init_obs,\n",
    "    train_state,\n",
    "    num_epochs=max_epochs,\n",
    "    num_steps_per_epoch=env.max_steps_in_episode,\n",
    "    num_envs=num_envs,\n",
    "    res_model_params=dummy_residual_params,\n",
    "    key=key_bptt,\n",
    ")\n",
    "time_train_compile = time.time() - time_start\n",
    "print(f\"Compile + Training time: {time_train_compile}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flightning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
