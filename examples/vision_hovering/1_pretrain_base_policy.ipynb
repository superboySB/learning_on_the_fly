{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc48ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from functools import partial\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from orbax.checkpoint import PyTreeCheckpointer\n",
    "from flax.training.train_state import TrainState\n",
    "from tqdm import tqdm\n",
    "\n",
    "from lotf import LOTF_PATH\n",
    "from lotf.envs import HoveringFeaturesEnv, rollout\n",
    "from lotf.modules import MLP\n",
    "from lotf.utils.math import normalize\n",
    "from lotf.objects import Quadrotor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71dc545",
   "metadata": {},
   "source": [
    "# Training a Feature-Based Hovering Policy With BPTT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc81526",
   "metadata": {},
   "source": [
    "## 1. Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8850bd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "key = jax.random.key(seed)\n",
    "key_init, key_bptt = jax.random.split(key, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69649248",
   "metadata": {},
   "source": [
    "## 2. Define Simulation Dynamics Config and Training Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e14b8e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulation dynamics config\n",
    "sim_dyn_config = {\n",
    "    \"use_high_fidelity\": False,          # whether to use high-fidelity dynamics in forward simulation\n",
    "    \"use_forward_residual\": False,       # whether to use residual dynamics in forward simulation\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3116fb",
   "metadata": {},
   "source": [
    "## 3. Create Quadrotor Object and Simulation Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15bce161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== env info ======\n",
      "action_dim: 4\n",
      "obs_dim: 82\n",
      "target hover goal: [1.5 0.  1.5]\n"
     ]
    }
   ],
   "source": [
    "# simulation parameters\n",
    "sim_dt = 0.02\n",
    "max_sim_time = 3.0\n",
    "\n",
    "# quadrotor object\n",
    "quad_obj = Quadrotor.from_name(\"example_quad\", sim_dyn_config)\n",
    "\n",
    "# simulation environment\n",
    "env = HoveringFeaturesEnv(\n",
    "    max_steps_in_episode=int(max_sim_time / sim_dt),\n",
    "    dt=sim_dt,\n",
    "    delay=0.04,\n",
    "    yaw_scale=1.0,\n",
    "    pitch_roll_scale=0.3,\n",
    "    velocity_std=2.,\n",
    "    omega_std=2.,\n",
    "    quad_obj=quad_obj,\n",
    "    reward_sharpness=5.0,\n",
    "    action_penalty_weight=0.5,\n",
    "    num_last_quad_states=15,\n",
    "    skip_frames=3,\n",
    "    hover_target=[1.5, 0.0, 1.5],\n",
    ")\n",
    "\n",
    "# get dimensions\n",
    "action_dim = env.action_space.shape[0]\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "\n",
    "print(\"====== env info ======\")\n",
    "print(f\"action_dim: {action_dim}\")\n",
    "print(f\"obs_dim: {obs_dim}\")\n",
    "print(f\"target hover goal: {env.goal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4340d218",
   "metadata": {},
   "source": [
    "## 4. Create Policy Network, Optimizer, and Train State (For Data Collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9e6ae57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy network and init parameters for data collection\n",
    "policy_net = MLP(\n",
    "    [obs_dim, 512, 512, action_dim],\n",
    "    initial_scale=1.0,\n",
    "    action_bias=env.hovering_action,\n",
    ")\n",
    "policy_params = policy_net.initialize(key_init)\n",
    "\n",
    "# dummy optimizer and train state for data collection\n",
    "tx = optax.adam(0)\n",
    "train_state = TrainState.create(\n",
    "    apply_fn=policy_net.apply, params=policy_params, tx=tx\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c691b6",
   "metadata": {},
   "source": [
    "## 5. Load Dummy Residual Dynamics Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2d806de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Since we are training a base policy, we do not actually use the residual dynamics for forward sim or backprop\n",
    "# However, we simply load a dummy residual dynamics model to satisfy the simulation environment requirements\n",
    "\n",
    "path = LOTF_PATH + \"/../checkpoints/residual_dynamics/dummy_params\"\n",
    "ckptr = PyTreeCheckpointer()\n",
    "dummy_residual_params = ckptr.restore(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041f685b",
   "metadata": {},
   "source": [
    "## 6. Collect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "074a3872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rollout time: 10.057557344436646\n"
     ]
    }
   ],
   "source": [
    "def collect_data(env, policy, num_rollouts, key):\n",
    "    parallel_rollout = jax.vmap(\n",
    "        partial(rollout, real_step=True, num_steps=1000),\n",
    "        in_axes=(None, 0, None, None),\n",
    "    )\n",
    "    rollout_keys = jax.random.split(key, num_rollouts)\n",
    "    transitions = parallel_rollout(env, rollout_keys, policy, dummy_residual_params)\n",
    "    return transitions\n",
    "\n",
    "def policy_collection(obs, key):\n",
    "    return train_state.apply_fn(train_state.params, obs)\n",
    "\n",
    "### collect rollout data\n",
    "time_rollout = time.time()\n",
    "transitions = collect_data(env, policy_collection, 100, jax.random.key(3))\n",
    "time_rollout = time.time() - time_rollout\n",
    "print(f\"Rollout time: {time_rollout}\")\n",
    "\n",
    "### create dataset \n",
    "# inputs: observations\n",
    "observations = transitions.obs\n",
    "observations = jnp.reshape(observations, (-1, observations.shape[-1]))\n",
    "\n",
    "# targets: quadrotor state\n",
    "p = transitions.state.quadrotor_state.p\n",
    "# normalize the position\n",
    "p = normalize(p, env.world_box.min, env.world_box.max)\n",
    "R = transitions.state.quadrotor_state.R\n",
    "v = transitions.state.quadrotor_state.v\n",
    "v = normalize(v, env.v_min, env.v_max)\n",
    "# flatten the last axis of R\n",
    "R = jnp.reshape(R, (*R.shape[:-2], -1))\n",
    "# concatenate the states\n",
    "targets = jnp.concatenate([p, R, v], axis=-1)\n",
    "targets = jnp.reshape(targets, (-1, targets.shape[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f915433d",
   "metadata": {},
   "source": [
    "## 7. Pretrain Policy Network on State Prediction Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f89cc27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:29<00:00, 17.09it/s]\n"
     ]
    }
   ],
   "source": [
    "@jax.jit\n",
    "def train_step(state: TrainState, obs, targets):\n",
    "    def loss_fn(params):\n",
    "        preds = state.apply_fn(params, obs)\n",
    "        loss = jnp.mean(jnp.abs(preds - targets))\n",
    "        return loss\n",
    "\n",
    "    grads = jax.grad(loss_fn)(state.params)  # Compute gradients\n",
    "    new_state = state.apply_gradients(grads=grads)  # Update parameters\n",
    "    return new_state\n",
    "\n",
    "def train_loop(state, observations, targets, epochs=100, batch_size=32):\n",
    "    dataset_size = observations.shape[0]\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        # Shuffle the data at the start of each epoch\n",
    "        perm = jax.random.permutation(jax.random.PRNGKey(epoch), dataset_size)\n",
    "        obs_shuffled = observations[perm]\n",
    "        targets_shuffled = targets[perm]\n",
    "\n",
    "        # Iterate over the dataset in batches\n",
    "        for i in range(0, dataset_size, batch_size):\n",
    "            batch_obs = obs_shuffled[i : i + batch_size]\n",
    "            batch_targets = targets_shuffled[i : i + batch_size]\n",
    "\n",
    "            # Perform a training step\n",
    "            state = train_step(state, batch_obs, batch_targets)\n",
    "\n",
    "    return state\n",
    "\n",
    "### create predictor MLP\n",
    "predictor = MLP([obs_dim, 512, 512, targets.shape[-1]], initial_scale=.1)\n",
    "predictor_params = predictor.initialize(jax.random.PRNGKey(0))\n",
    "\n",
    "tx_predictor = optax.adam(1e-3)\n",
    "train_state_predictor = TrainState.create(\n",
    "    apply_fn=predictor.apply, params=predictor_params, tx=tx_predictor\n",
    ")\n",
    "epochs = 500\n",
    "batch_size = 1024\n",
    "\n",
    "# train the state predictor\n",
    "train_state_predictor_new = train_loop(train_state_predictor, observations,\n",
    "                                    targets,\n",
    "                                    epochs, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e30d61f",
   "metadata": {},
   "source": [
    "## 8. Save the Pretrained Policy Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "faffe220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy saved successfully!\n"
     ]
    }
   ],
   "source": [
    "policy_name = \"my_vision_hovering_pre_params\"\n",
    "\n",
    "# copy the trained policy parameters\n",
    "policy_params['params']['Dense_0'] = train_state_predictor_new.params['params']['Dense_0']\n",
    "policy_params['params']['Dense_1'] = train_state_predictor_new.params['params']['Dense_1']\n",
    "\n",
    "path = LOTF_PATH + \"/../checkpoints/policy/\" + policy_name\n",
    "ckptr = PyTreeCheckpointer()\n",
    "ckptr.save(path, policy_params)\n",
    "print(f\"Policy saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lotf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
