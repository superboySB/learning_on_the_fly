{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc48ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import jax\n",
    "import optax\n",
    "from orbax.checkpoint import PyTreeCheckpointer\n",
    "from flax.training.train_state import TrainState\n",
    "\n",
    "from lotf import LOTF_PATH\n",
    "from lotf.algos import bptt\n",
    "from lotf.envs import HoveringFeaturesEnv\n",
    "from lotf.envs.wrappers import LogWrapper, VecEnv\n",
    "from lotf.modules import MLP\n",
    "from lotf.objects import Quadrotor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71dc545",
   "metadata": {},
   "source": [
    "# Finetuning a Trained Feature-Based Hovering Policy With BPTT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc81526",
   "metadata": {},
   "source": [
    "## 1. Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8850bd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "key = jax.random.key(seed)\n",
    "key_init, key_bptt = jax.random.split(key, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69649248",
   "metadata": {},
   "source": [
    "## 2. Define Simulation Dynamics Config and Training Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e14b8e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulation dynamics config\n",
    "sim_dyn_config = {\n",
    "    \"use_high_fidelity\": False,          # whether to use high-fidelity dynamics in forward simulation\n",
    "    \"use_forward_residual\": False,       # whether to use residual dynamics in forward simulation\n",
    "}\n",
    "\n",
    "# training parameters\n",
    "num_envs = 300\n",
    "max_epochs = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3116fb",
   "metadata": {},
   "source": [
    "## 3. Create Quadrotor Object and Simulation Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15bce161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== env info ======\n",
      "action_dim: 4\n",
      "obs_dim: 82\n",
      "target hover goal: [1.5 0.  1.5]\n"
     ]
    }
   ],
   "source": [
    "# simulation parameters\n",
    "sim_dt = 0.02\n",
    "max_sim_time = 3.0\n",
    "\n",
    "# quadrotor object\n",
    "quad_obj = Quadrotor.from_name(\"example_quad\", sim_dyn_config)\n",
    "\n",
    "# simulation environment\n",
    "env = HoveringFeaturesEnv(\n",
    "    max_steps_in_episode=int(max_sim_time / sim_dt),\n",
    "    dt=sim_dt,\n",
    "    delay=0.04,\n",
    "    yaw_scale=1.0,\n",
    "    pitch_roll_scale=0.1,\n",
    "    velocity_std=0.1,\n",
    "    omega_std=0.1,\n",
    "    quad_obj=quad_obj,\n",
    "    reward_sharpness=2.0,\n",
    "    action_penalty_weight=0.5,\n",
    "    num_last_quad_states=15,\n",
    "    skip_frames=3,\n",
    "    margin=0.5,\n",
    "    hover_target=[1.5, 0.0, 1.5],\n",
    ")\n",
    "\n",
    "# get dimensions\n",
    "action_dim = env.action_space.shape[0]\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "\n",
    "# apply additional wrappers\n",
    "env = LogWrapper(env)\n",
    "env = VecEnv(env)\n",
    "\n",
    "print(\"====== env info ======\")\n",
    "print(f\"action_dim: {action_dim}\")\n",
    "print(f\"obs_dim: {obs_dim}\")\n",
    "print(f\"target hover goal: {env.goal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4340d218",
   "metadata": {},
   "source": [
    "## 4. Load Policy Parameters, Create Optimizer and Train State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9e6ae57",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_name = \"vision_hovering_params\"\n",
    "\n",
    "# policy network and init parameters\n",
    "policy_net = MLP(\n",
    "    [obs_dim, 512, 512, action_dim],\n",
    "    initial_scale=0.01,\n",
    "    action_bias=env.hovering_action,\n",
    ")\n",
    "path = LOTF_PATH + \"/../checkpoints/policy/\" + policy_name\n",
    "ckptr = PyTreeCheckpointer()\n",
    "policy_params = ckptr.restore(path)\n",
    "\n",
    "# optimizer\n",
    "scheduler = optax.cosine_decay_schedule(1e-3, max_epochs)\n",
    "tx = optax.adam(scheduler)\n",
    "\n",
    "# train state object\n",
    "train_state = TrainState.create(\n",
    "    apply_fn=policy_net.apply, params=policy_params, tx=tx\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c691b6",
   "metadata": {},
   "source": [
    "## 5. Load Residual Dynamics Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2d806de",
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_dynamics_name = \"example_params\"\n",
    "\n",
    "path = LOTF_PATH + \"/../checkpoints/residual_dynamics/\" + residual_dynamics_name\n",
    "ckptr = PyTreeCheckpointer()\n",
    "dummy_residual_params = ckptr.restore(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041f685b",
   "metadata": {},
   "source": [
    "## 6. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f89cc27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Grad max: 0.1935\n",
      "Episode: 0, Loss: 1.06\n",
      "Episode: 10, Grad max: 0.8374\n",
      "Episode: 10, Loss: 2.30\n",
      "Episode: 20, Grad max: 0.4450\n",
      "Episode: 20, Loss: 1.59\n",
      "Episode: 30, Grad max: 0.4201\n",
      "Episode: 30, Loss: 1.28\n",
      "Episode: 40, Grad max: 0.2799\n",
      "Episode: 40, Loss: 1.10\n",
      "Episode: 50, Grad max: 0.2603\n",
      "Episode: 50, Loss: 1.00\n",
      "Episode: 60, Grad max: 0.2331\n",
      "Episode: 60, Loss: 1.04\n",
      "Episode: 70, Grad max: 0.1395\n",
      "Episode: 70, Loss: 1.02\n",
      "Episode: 80, Grad max: 0.1219\n",
      "Episode: 80, Loss: 0.98\n",
      "Episode: 90, Grad max: 0.1135\n",
      "Episode: 90, Loss: 0.94\n",
      "Episode: 100, Grad max: 0.1169\n",
      "Episode: 100, Loss: 0.97\n",
      "Episode: 110, Grad max: 0.1180\n",
      "Episode: 110, Loss: 0.92\n",
      "Episode: 120, Grad max: 0.1710\n",
      "Episode: 120, Loss: 0.91\n",
      "Episode: 130, Grad max: 0.0906\n",
      "Episode: 130, Loss: 0.93\n",
      "Episode: 140, Grad max: 0.0786\n",
      "Episode: 140, Loss: 0.92\n",
      "Episode: 150, Grad max: 0.0966\n",
      "Episode: 150, Loss: 0.87\n",
      "Episode: 160, Grad max: 0.0682\n",
      "Episode: 160, Loss: 0.90\n",
      "Episode: 170, Grad max: 0.0814\n",
      "Episode: 170, Loss: 0.89\n",
      "Episode: 180, Grad max: 0.0503\n",
      "Episode: 180, Loss: 0.96\n",
      "Episode: 190, Grad max: 0.0981\n",
      "Episode: 190, Loss: 0.94\n",
      "Compile + Training time: 27.07131576538086\n"
     ]
    }
   ],
   "source": [
    "# intialize environments\n",
    "key_bptt, key_ = jax.random.split(key_bptt)\n",
    "key_reset = jax.random.split(key_, num_envs)\n",
    "init_env_state, init_obs = env.reset(key_reset, None)\n",
    "\n",
    "# training loop\n",
    "time_start = time.time()\n",
    "res_dict = bptt.train(\n",
    "    env,\n",
    "    init_env_state,\n",
    "    init_obs,\n",
    "    train_state,\n",
    "    num_epochs=max_epochs,\n",
    "    num_steps_per_epoch=env.max_steps_in_episode,\n",
    "    num_envs=num_envs,\n",
    "    res_model_params=dummy_residual_params,\n",
    "    key=key_bptt,\n",
    ")\n",
    "time_train_compile = time.time() - time_start\n",
    "print(f\"Compile + Training time: {time_train_compile}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lotf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
